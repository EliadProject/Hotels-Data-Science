{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('clustering_spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Hotels Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels_data = spark.read.csv(\"hotels_data.csv\", header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converts String to Dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2015, 8, 12)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import DateType, IntegerType\n",
    "\n",
    "# Converts string to date\n",
    "def str_to_date(str):    \n",
    "    return datetime.strptime(str, '%m/%d/%Y %H:%M')\n",
    "\n",
    "# convert a regular function to pyspark function\n",
    "udf_strToDate = udf(str_to_date, DateType())\n",
    "\n",
    "# convert \"checking_date\" and \"snapshot_date\" to date types\n",
    "hotels_data_with_dates = hotels_data.withColumn(\"checkin_date\", udf_strToDate(col(\"Checkin Date\")))\n",
    "hotels_data_with_dates = hotels_data_with_dates.withColumn(\"snapshot_date\", udf_strToDate(col(\"Snapshot Date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin 187848\n",
      "new 169340\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# group by hotel name and count, take the first 150 hotels with the biggest count \n",
    "count_by_hotel_names = hotels_data_with_dates.groupBy('Hotel Name').count().sort(desc('count')).limit(150)\n",
    "\n",
    "# get a list of the first 150 hotel names \n",
    "first_150_hotel_names = count_by_hotel_names.toPandas()['Hotel Name'].tolist()\n",
    "\n",
    "# filter hotels_data to include records from the 150 hotel names\n",
    "hotels_150_data = hotels_data_with_dates.filter(col('Hotel Name').isin(first_150_hotel_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by checkin and count, take the first 40 with biggest count\n",
    "count_by_checkin = hotels_150_data.groupBy('checkin_date').count().sort(desc('count')).limit(40)\n",
    "\n",
    "# get a list of the most common 40 checkin dates\n",
    "first_40_checkin = count_by_checkin.toPandas()['checkin_date'].tolist()\n",
    "\n",
    "# filter hotels data by the 40 most common dates\n",
    "hotels_by_40_checkin = hotels_150_data.filter(col('checkin_date').isin(first_40_checkin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Column<b'Hotel Name'>, Column<b'Checkin Date'>, 1, 9223372036854775807],\n",
       " [Column<b'Hotel Name'>, Column<b'Checkin Date'>, 2, 9223372036854775807],\n",
       " [Column<b'Hotel Name'>, Column<b'Checkin Date'>, 3, 9223372036854775807],\n",
       " [Column<b'Hotel Name'>, Column<b'Checkin Date'>, 4, 9223372036854775807]]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating unique list for Hotel Name - Checkin Date - Discount code dummy combination generating \n",
    "unique_hotels_names = hotels_by_40_checkin.select('Hotel Name').distinct().collect()\n",
    "unique_hotels_names_list = [(row['Hotel Name']) for row in unique_hotels_names]\n",
    "\n",
    "unique_checkins =  hotels_by_40_checkin.select(\"checkin_date\").distinct().collect()\n",
    "unique_checkins_list = [(row['checkin_date']) for row in unique_checkins]\n",
    "\n",
    "unique_discount_code =  [1,2,3,4]\n",
    "\n",
    "# Create records with max price for each discount code for each date\n",
    "synth_data = []\n",
    "import sys\n",
    "for x in unique_hotels_names_list:\n",
    "    for y in unique_checkins_list:\n",
    "        for z in unique_discount_code:\n",
    "            synth_data.append([x, y ,z, sys.maxsize])\n",
    "\n",
    "#Making the schema of synth_data\n",
    "from pyspark.sql.types import *\n",
    "cSchema = StructType([StructField(\"Hotel Name\", StringType()),StructField(\"checkin_date\", DateType()),StructField(\"Discount Code\",  IntegerType()),StructField(\"min(Discount Price)\", LongType())])\n",
    "\n",
    "#Creating dummy df\n",
    "dummy_df = spark.createDataFrame(synth_data, schema=cSchema)\n",
    "\n",
    "sliced_df = hotels_by_40_checkin.select('Hotel Name', 'checkin_date','Discount Code', 'Discount Price')\n",
    "\n",
    "# joining dummy data with grouped data \n",
    "hotel_chekin_discountCode = sliced_df.union(dummy_df)\n",
    "\n",
    "# group by Checkin - Hotel - Discount Code\n",
    "hotel_chekin_discountCode = hotel_chekin_discountCode.groupBy('Hotel name','checkin_date', 'Discount Code').min('Discount Price')\n",
    "\n",
    "#replacing sys.max with -1 \n",
    "# hotel_chekin_discountCode = hotel_chekin_discountCode.replace(sys.maxsize, -1)\n",
    "\n",
    "#sorting date\n",
    "# hotel_chekin_discountCode = hotel_chekin_discountCode.orderBy(['Hotel name','checkin_date','Discount Code'])\n",
    "\n",
    "hotel_chekin_discountCode.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split to two groups: one with price of -1, one with greater than -1\n",
    "groupMinus = hotel_chekin_discountCode.filter(col('min(Discount Price)') == -1)\n",
    "groupGreater = hotel_chekin_discountCode.filter(col('min(Discount Price)') > -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "groupGreater.createOrReplaceTempView(\"normalization\")\n",
    "\n",
    "#Creating SQLContext for SQL converting\n",
    "sqlContext = SQLContext(spark)\n",
    "    \n",
    "#Preparing data for normalization\n",
    "dataFrame = sqlContext.table(\"normalization\")\n",
    "#Partioning data for groups so we can applay function on groups\n",
    "windowSpec = Window.partitionBy(groupGreater['Hotel name'])  \n",
    "\n",
    "#Maximun of group\n",
    "minv = func.min(dataFrame['min(Discount Price)']).over(windowSpec)\n",
    "#Minimun of group\n",
    "maxv = func.max(dataFrame['min(Discount Price)']).over(windowSpec)\n",
    "#Normalize function\n",
    "normalize =  ((dataFrame['min(Discount Price)'] - minv) / (maxv - minv) * 100)\n",
    "\n",
    "normalized_df = dataFrame.select(\n",
    "  dataFrame['Hotel Name'],\n",
    "  dataFrame['checkin_date'],\n",
    "  dataFrame['Discount Code'],\n",
    "  normalize.alias(\"Normal\"))\n",
    "\n",
    "#Changing column name for same schema\n",
    "groupMinus = groupMinus.withColumnRenamed('min(Discount Price)','Normal')\n",
    "\n",
    "#Union all data frames with sorting\n",
    "normalized_df = normalized_df.union(groupMinus)\n",
    "normalized_df = normalized_df.orderBy(['Hotel name','checkin_date','Discount Code'])\n",
    "normalized_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "groupGreater.groupBy('Hotel name').agg(F.collect_list('min(Discount Price)').alias('Discount Price')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
