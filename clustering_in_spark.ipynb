{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('clustering_spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Hotels Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels_data = spark.read.csv(\"hotels_data.csv\", header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converts String to Dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2015, 8, 12)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import DateType, IntegerType\n",
    "\n",
    "# Converts string to date\n",
    "def str_to_date(str):    \n",
    "    return datetime.strptime(str, '%m/%d/%Y %H:%M')\n",
    "\n",
    "# convert a regular function to pyspark function\n",
    "udf_strToDate = udf(str_to_date, DateType())\n",
    "\n",
    "# convert \"checking_date\" and \"snapshot_date\" to date types\n",
    "hotels_data_with_dates = hotels_data.withColumn(\"checkin_date\", udf_strToDate(col(\"Checkin Date\")))\n",
    "hotels_data_with_dates = hotels_data_with_dates.withColumn(\"snapshot_date\", udf_strToDate(col(\"Snapshot Date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin 187848\n",
      "new 169340\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# group by hotel name and count, take the first 150 hotels with the biggest count \n",
    "count_by_hotel_names = hotels_data_with_dates.groupBy('Hotel Name').count().sort(desc('count')).limit(150)\n",
    "\n",
    "# get a list of the first 150 hotel names \n",
    "first_150_hotel_names = count_by_hotel_names.toPandas()['Hotel Name'].tolist()\n",
    "\n",
    "# filter hotels_data to include records from the 150 hotel names\n",
    "hotels_150_data = hotels_data_with_dates.filter(col('Hotel Name').isin(first_150_hotel_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by checkin and count, take the first 40 with biggest count\n",
    "count_by_checkin = hotels_150_data.groupBy('checkin_date').count().sort(desc('count')).limit(40)\n",
    "\n",
    "# get a list of the most common 40 checkin dates\n",
    "first_40_checkin = count_by_checkin.toPandas()['checkin_date'].tolist()\n",
    "\n",
    "# filter hotels data by the 40 most common dates\n",
    "hotels_by_40_checkin = hotels_150_data.filter(col('checkin_date').isin(first_40_checkin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Column<b'Hotel Name'>, Column<b'Checkin Date'>, 1, 9223372036854775807],\n",
       " [Column<b'Hotel Name'>, Column<b'Checkin Date'>, 2, 9223372036854775807],\n",
       " [Column<b'Hotel Name'>, Column<b'Checkin Date'>, 3, 9223372036854775807],\n",
       " [Column<b'Hotel Name'>, Column<b'Checkin Date'>, 4, 9223372036854775807]]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_hotels_names = hotels_by_40_checkin.select(\"Hotel Name\").distinct()\n",
    "unique_checkins = hotels_by_40_checkin.select(\"Checkin Date\").distinct()\n",
    "unique_discount_code = [1, 2, 3, 4]\n",
    "\n",
    "# creating default data - all combination : checking -hotel - discount code\n",
    "import sys\n",
    "\n",
    "combs = []\n",
    "for x in unique_hotels_names:\n",
    "    for y in unique_checkins:\n",
    "        for z in unique_discount_code:\n",
    "            combs.append([x, y, z, sys.maxsize])\n",
    "            \n",
    "new_df = spark.createDataFrame(combs, [\"Hotel Name\", \"Checkin Date\", \"Discount Code\", \"Discount Price\"])\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
